{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c72a3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc98bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8e1ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/16 23:16:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .config(\"spark.driver.memory\", \"10g\")\\\n",
    "        .appName(\"BucketingBy\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4be15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd953cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_file = \"/Users/sateeshreddypatlolla/Downloads/SQL/CSV/Mini_Project_2/Order_data.csv\"\n",
    "df_orders = spark.read.csv(orders_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49672711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+---------------+\n",
      "|OrderID|CustomerID|ProductID|OrderDate |QuantityOrdered|\n",
      "+-------+----------+---------+----------+---------------+\n",
      "|1      |1         |68       |2012-08-14|1              |\n",
      "|2      |1         |22       |2012-08-14|6              |\n",
      "|3      |1         |66       |2012-08-14|4              |\n",
      "|4      |1         |42       |2012-08-15|6              |\n",
      "|5      |1         |53       |2012-08-15|2              |\n",
      "+-------+----------+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- OrderDate: date (nullable = true)\n",
      " |-- QuantityOrdered: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders.show(5, False)\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e340227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_file = \"/Users/sateeshreddypatlolla/Downloads/SQL/CSV/Mini_Project_2/Product_data.csv\"\n",
    "df_products = spark.read.csv(products_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd250ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+----------------+-----------------+\n",
      "|ProductID|ProductName      |ProductUnitPrice|ProductCategoryID|\n",
      "+---------+-----------------+----------------+-----------------+\n",
      "|1        |Alice Mutton     |39.0            |6                |\n",
      "|2        |Aniseed Syrup    |10.0            |2                |\n",
      "|3        |Boston Crab Meat |18.4            |8                |\n",
      "|4        |Camembert Pierrot|34.0            |4                |\n",
      "|5        |Carnarvon Tigers |62.5            |8                |\n",
      "+---------+-----------------+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- ProductUnitPrice: double (nullable = true)\n",
      " |-- ProductCategoryID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_products.show(5, False)\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1899cc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products.select(\"ProductID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154b7769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621806"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders.select(\"OrderID\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce6917ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details = (\n",
    "    df_orders.join(\n",
    "        df_products,\n",
    "        on=\"ProductID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327081ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [ProductID#19, OrderID#17, CustomerID#18, OrderDate#20, QuantityOrdered#21, ProductName#72, ProductUnitPrice#73, ProductCategoryID#74]\n",
      "   +- SortMergeJoin [ProductID#19], [ProductID#71], Inner\n",
      "      :- Sort [ProductID#19 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(ProductID#19, 200), ENSURE_REQUIREMENTS, [plan_id=259]\n",
      "      :     +- Filter isnotnull(ProductID#19)\n",
      "      :        +- FileScan csv [OrderID#17,CustomerID#18,ProductID#19,OrderDate#20,QuantityOrdered#21] Batched: false, DataFilters: [isnotnull(ProductID#19)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/sateeshreddypatlolla/Downloads/SQL/CSV/Mini_Project_2/Orde..., PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<OrderID:int,CustomerID:int,ProductID:int,OrderDate:date,QuantityOrdered:int>\n",
      "      +- Sort [ProductID#71 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(ProductID#71, 200), ENSURE_REQUIREMENTS, [plan_id=260]\n",
      "            +- Filter isnotnull(ProductID#71)\n",
      "               +- FileScan csv [ProductID#71,ProductName#72,ProductUnitPrice#73,ProductCategoryID#74] Batched: false, DataFilters: [isnotnull(ProductID#71)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/sateeshreddypatlolla/Downloads/SQL/CSV/Mini_Project_2/Prod..., PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,ProductName:string,ProductUnitPrice:double,ProductCategoryID:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df35582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621806"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ae7dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_orders\n",
    "    .write.bucketBy(25, col=\"ProductID\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"orders_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f60c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_products\n",
    "    .write.bucketBy(10, \"ProductID\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"products_bucketed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b62255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_bucketed = spark.table(\"orders_bucketed\")\n",
    "df_products_bucketed = spark.table(\"products_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b1d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_product_details_bucketed = (\n",
    "    df_orders_bucketed.join(\n",
    "        df_products_bucketed,\n",
    "        on=\"ProductID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99c98c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [ProductID#206, OrderID#204, CustomerID#205, OrderDate#207, QuantityOrdered#208, ProductName#215, ProductUnitPrice#216, ProductCategoryID#217]\n",
      "   +- SortMergeJoin [ProductID#206], [ProductID#214], Inner\n",
      "      :- Sort [ProductID#206 ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(ProductID#206)\n",
      "      :     +- FileScan parquet spark_catalog.default.orders_bucketed[OrderID#204,CustomerID#205,ProductID#206,OrderDate#207,QuantityOrdered#208] Batched: true, Bucketed: true, DataFilters: [isnotnull(ProductID#206)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/sateeshreddypatlolla/Downloads/SparkCourse/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<OrderID:int,CustomerID:int,ProductID:int,OrderDate:date,QuantityOrdered:int>, SelectedBucketsCount: 25 out of 25\n",
      "      +- Sort [ProductID#214 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(ProductID#214, 25), ENSURE_REQUIREMENTS, [plan_id=616]\n",
      "            +- Filter isnotnull(ProductID#214)\n",
      "               +- FileScan parquet spark_catalog.default.products_bucketed[ProductID#214,ProductName#215,ProductUnitPrice#216,ProductCategoryID#217] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(ProductID#214)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/sateeshreddypatlolla/Downloads/SparkCourse/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(ProductID)], ReadSchema: struct<ProductID:int,ProductName:string,ProductUnitPrice:double,ProductCategoryID:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2da61c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621806"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_product_details_bucketed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb014aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
